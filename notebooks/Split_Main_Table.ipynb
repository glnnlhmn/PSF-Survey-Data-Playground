{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655c950522dc3107",
   "metadata": {},
   "source": [
    "# Split Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T02:36:56.069581Z",
     "start_time": "2024-11-30T02:36:56.066039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e13220054d3fa",
   "metadata": {},
   "source": [
    "## Define Variables and Read CSV File\n",
    "Variables are being used to make the management of the path to the file easier.\n",
    "\n",
    "### Local Change\n",
    "You will need to update the value of **project_path** to correspond with the path to where you cloned the project.\n",
    "\n",
    "#### Pandas and pyarrow\n",
    "Using the PyArrow backend for pandas offers several benefits:\n",
    "\n",
    "1. **Improved Performance**: PyArrow's columnar storage format optimizes data access and retrieval, leading to faster processing speeds.\n",
    "2. **Enhanced Memory Efficiency**: PyArrow is more memory-efficient, especially when handling string data types, compared to the default NumPy backend.\n",
    "3. **Better Interoperability**: PyArrow facilitates seamless data exchange between different programming languages, reducing the overhead of data conversion.\n",
    "4. **Extended Data Type Support**: PyArrow supports a wider range of data types and provides better handling of missing data (NA values) across all types.\n",
    "\n",
    "These advantages make PyArrow a powerful alternative to the traditional NumPy backend in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2031477666a2b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T02:36:59.555300Z",
     "start_time": "2024-11-30T02:36:56.098881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the year as a variable\n",
    "year = 2023\n",
    "\n",
    "# Define the project path and subdirectories\n",
    "project_path = 'C:/Users/glenn/ws/PSF-Survey-Data-Playground/'\n",
    "raw_path = f'data/{year}/raw/'\n",
    "clean_path = f'data/{year}/cleaned/'\n",
    "\n",
    "# Load the dataset using the specified path and pyarrow dtype backend\n",
    "df = pd.read_csv(f'{project_path}{raw_path}{year}_sharing_data_outside.csv', dtype_backend=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9c6b51440d3e7",
   "metadata": {},
   "source": [
    "## Adding main_id as a Primary Key and Using it as a Foreign Key in Split Tables\n",
    "\n",
    "To ensure data integrity and establish clear relationships between tables, I am adding main_id as the primary key in the main table. This unique identifier will serve as a reference point for all related records. By splitting the main table into multiple tables, each new table will include main_id as a foreign key. This approach allows for efficient data organization, easy retrieval, and maintains referential integrity across the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b218dc0d079413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T02:36:59.563785Z",
     "start_time": "2024-11-30T02:36:59.560227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a primary key column\n",
    "df.insert(0, 'main_id', range(1, len(df) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10368e5df37d4c11",
   "metadata": {},
   "source": [
    "## Splitting the Table\n",
    "\n",
    "The current table contains over 400 columns, making it difficult to manage and query efficiently. During exploration, I observed a hidden convention in the column names, where each column follows the format `table.column_name`. To improve the table's usability and maintainability, I will split it into multiple tables based on these conventions. Each new table will be created only if it contains at least six columns. This approach will streamline data management and enhance query performance.\n",
    "\n",
    "To allow for dynamic naming of columns, a dictionary of all the new dataframes will be created. The `main_id` will be included in each dataframe to maintain referential integrity. The original dataframe `df` will not be modified.\n",
    "\n",
    "Any column name that does not contain a `.` will be placed in the `main_df`, as well as all column groupings containing five or fewer columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3335d5ffdf843d5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T02:37:00.075309Z",
     "start_time": "2024-11-30T02:36:59.582501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract column names by converting to a list\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Create a list to store the table names\n",
    "table_names = []\n",
    "\n",
    "# Derive table names from column names\n",
    "for col in column_names:\n",
    "    if '.' in col:\n",
    "        table_name = col.split('.')[0]\n",
    "    else:\n",
    "        table_name = 'main'\n",
    "    table_names.append(table_name)\n",
    "\n",
    "# Create a DataFrame with column names and table names\n",
    "table_col_xref_df = pd.DataFrame({\n",
    "    'Column Name': column_names,\n",
    "    'Table Name': table_names\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6fcd4971f5399f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T02:37:00.394325Z",
     "start_time": "2024-11-30T02:37:00.095953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get count of each unique table name\n",
    "unique_table_counts = table_col_xref_df['Table Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01b2daf91d8df3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T02:37:00.858312Z",
     "start_time": "2024-11-30T02:37:00.417651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through each unique table name and its count\n",
    "for table_name, count in unique_table_counts.items():\n",
    "    if count < 6 and table_name != 'main':\n",
    "\n",
    "        # Reassign the table name to 'main' for these columns\n",
    "        table_col_xref_df.loc[table_col_xref_df['Table Name'] == table_name, 'Table Name'] = 'main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "118a147634de65f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T02:37:01.246955Z",
     "start_time": "2024-11-30T02:37:00.877132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update the counts after smaller tables are placed into the main table\n",
    "# This ensures the variable is updated for completeness and potential future use,\n",
    "# even though it is no longer used in the current code\n",
    "unique_table_counts = table_col_xref_df['Table Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "408c1c732f696aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T02:37:01.704019Z",
     "start_time": "2024-11-30T02:37:01.341905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get unique table names\n",
    "unique_table_names = table_col_xref_df['Table Name'].unique()\n",
    "\n",
    "# Create a dictionary to store the resulting DataFrames\n",
    "# Create a dictionary to store the resulting DataFrames\n",
    "dataframes = {}\n",
    "for table_name in unique_table_names:\n",
    "    columns = ['main_id'] + table_col_xref_df[table_col_xref_df['Table Name'] == table_name]['Column Name'].tolist()\n",
    "    dataframes[f\"{table_name}_df\"] = df[columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448974d056bb5be",
   "metadata": {},
   "source": [
    "Here's a markdown narrative to describe the process:\n",
    "\n",
    "---\n",
    "\n",
    "## Updating Column Names and Cleaning DataFrames\n",
    "\n",
    "To streamline the column names and clean the dataframes, I followed these steps:\n",
    "\n",
    "1. **Update Column Names**:\n",
    "    - For each dataframe in the dictionary, except for `main_df`, I updated the column names to only include the values after the dot (`.`). This was done to simplify the column names and make them more readable.\n",
    "    - The `main_id` column was retained as is.\n",
    "    - The updated column names were assigned back to the dataframe.\n",
    "\n",
    "2. **Drop Rows with All NaN Values**:\n",
    "    - For each dataframe, I dropped rows where all columns, except for `main_id`, contained NaN values. This step ensured that only rows with meaningful data were retained.\n",
    "\n",
    "3. **Save DataFrames to CSV Files**:\n",
    "    - Finally, I saved each cleaned dataframe to a CSV file. The files were saved to the specified project path and clean path, with the dataframe name used as the filename.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa291313a6e47731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T02:37:05.305238Z",
     "start_time": "2024-11-30T02:37:01.762802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update all the column names to just have the values after the dot, with the except 'main'\n",
    "for name, dataframe in dataframes.items():\n",
    "    if name != \"main_df\":\n",
    "        new_columns = ['main_id'] + [col.split('.')[-1] if '.' in col else col for col in dataframe.columns[1:]]\n",
    "        dataframe.columns = new_columns\n",
    "\n",
    "for name, dataframe in dataframes.items():\n",
    "    # Drop rows where all columns, except 'main_id', are NaN\n",
    "    dataframes[name] = dataframe.dropna(subset=dataframe.columns.difference(['main_id']), how='all')\n",
    "\n",
    "# Save the resulting DataFrames to CSV files\n",
    "for name, dataframe in dataframes.items():\n",
    "    dataframe.to_csv(f'{project_path}{clean_path}{name}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
